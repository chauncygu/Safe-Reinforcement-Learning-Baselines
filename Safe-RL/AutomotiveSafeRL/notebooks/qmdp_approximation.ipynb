{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Belief State Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Recompiling stale cache file /mnt/c/Users/Maxime/wsl/.julia/compiled/v1.0/PedCar/NmDDZ.ji for PedCar [90cf7f26-d5c7-593d-a0e1-4a8367407571]\n",
      "└ @ Base loading.jl:1184\n",
      "┌ Warning: Package PedCar does not have AutomotivePOMDPs in its dependencies:\n",
      "│ - If you have PedCar checked out for development and have\n",
      "│   added AutomotivePOMDPs as a dependency but haven't updated your primary\n",
      "│   environment's manifest file, try `Pkg.resolve()`.\n",
      "│ - Otherwise you may need to report an issue with PedCar\n",
      "└ Loading AutomotivePOMDPs into PedCar from project dependency, future warnings for PedCar are suppressed.\n"
     ]
    }
   ],
   "source": [
    "using Random\n",
    "using Printf\n",
    "using StaticArrays\n",
    "using Flux\n",
    "using FileIO\n",
    "using BSON\n",
    "using JLD2\n",
    "using ProgressMeter\n",
    "using POMDPs\n",
    "using POMDPModelTools\n",
    "using POMDPSimulators\n",
    "using POMDPPolicies\n",
    "using BeliefUpdaters\n",
    "using DeepRL\n",
    "using DeepQLearning\n",
    "using LocalApproximationValueIteration\n",
    "using DiscreteValueIteration\n",
    "using AutomotiveDrivingModels\n",
    "using AutoViz\n",
    "using AutomotivePOMDPs\n",
    "using MDPModelChecking\n",
    "using PedCar\n",
    "using AutomotiveSensors\n",
    "using Reel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "animate_history (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "include(\"masking.jl\")\n",
    "include(\"util.jl\")\n",
    "include(\"masked_dqn.jl\")\n",
    "include(\"qmdp_approximation.jl\")\n",
    "include(\"render_helpers.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"training_scripts/RNNFiltering/RNNFiltering.jl\")\n",
    "using Main.RNNFiltering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = MersenneTwister(1)\n",
    "cam = FitToContentCamera(0.);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp = PedCarMDP(pos_res=2.0, vel_res=2., ped_birth=0.7, car_birth=0.7)\n",
    "pomdp = UrbanPOMDP(env=mdp.env,\n",
    "                   sensor = PerfectSensor(),\n",
    "#                     sensor = GaussianSensor(false_positive_rate=0.0, \n",
    "#                                             pos_noise = LinearNoise(min_noise=0.5, increase_rate=0.05), \n",
    "#                                             vel_noise = LinearNoise(min_noise=0.5, increase_rate=0.05)),\n",
    "                   ego_goal = LaneTag(2, 1),\n",
    "                   obs_dist = ObstacleDistribution(mdp.env, \n",
    "                                                   upper_obs_pres_prob=0., \n",
    "                                                   left_obs_pres_prob=1.0, \n",
    "                                                   right_obs_pres_prob=1.0),\n",
    "                   max_cars=1, \n",
    "                   max_peds=1, \n",
    "                   car_birth=0.1, \n",
    "                   ped_birth=0.1, \n",
    "                   max_obstacles=1, # no fixed obstacles\n",
    "                   lidar=false,\n",
    "                   ego_start=20,\n",
    "                   ΔT=0.1);\n",
    "\n",
    "# instantiate sub problems\n",
    "dqn_pomdp = deepcopy(pomdp)\n",
    "dqn_pomdp.max_obstacles = 0\n",
    "dqn_pomdp.models = pomdp.models\n",
    "\n",
    "rnn_pomdp = deepcopy(pomdp)\n",
    "rnn_pomdp.max_obstacles = 1\n",
    "rnn_pomdp.models = pomdp.models;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "function POMDPModelTools.generate_sori(pomdp::UrbanPOMDP, s::Scene, a::UrbanAction, rng::AbstractRNG)\n",
    "    sp, o, r = generate_sor(pomdp, s, a, rng)\n",
    "    return sp, o, r, deepcopy(pomdp.models)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load DRQN Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.99\n",
    "@load \"pc_util_processed.jld2\" qmat util pol\n",
    "safe_policy = ValueIterationPolicy(mdp, qmat, util, pol);\n",
    "mask = SafetyMask(mdp, safe_policy, threshold);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_file=\"training_scripts/drqn-log/log8/problem.bson\"\n",
    "weights_file=\"training_scripts/drqn-log/log8/weights.bson\"\n",
    "env_ = POMDPEnvironment(dqn_pomdp)\n",
    "dqn_policy = DeepQLearning.restore(env_, problem_file=problem_file, weights_file=weights_file)\n",
    "policy = MaskedNNPolicy(dqn_pomdp, dqn_policy, mask);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load RNN Belief Updater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_models = 5\n",
    "models = Vector{Chain}(undef, n_models)\n",
    "for i=1:n_models\n",
    "    models[i] = BSON.load(\"training_scripts/RNNFiltering/model_$(i)0.bson\")[:model] \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"qmdp_approximation.jl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simulation with RNN Belief Updater**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pomdp.sensor = GaussianSensor(false_positive_rate=0.0, \n",
    "                            pos_noise = LinearNoise(min_noise=0.5, increase_rate=0.05), \n",
    "                            vel_noise = LinearNoise(min_noise=0.5, increase_rate=0.05));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: PedCarRNNUpdater not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: PedCarRNNUpdater not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[1]:1"
     ]
    }
   ],
   "source": [
    "up = PedCarRNNUpdater(models, mdp, rnn_pomdp)\n",
    "reset_updater!(up)\n",
    "DeepQLearning.reset_hidden_state!(policy)\n",
    "s0 = initialstate(pomdp, rng)\n",
    "a0 = UrbanAction(1.0)\n",
    "o0 = generate_o(pomdp, s0, a0, s0, rng)\n",
    "b0 = update(up, PedCarRNNBelief(Vector{Vector{Float64}}(undef, n_models), o0), a0, o0);\n",
    "singleaction_policy = FunctionPolicy(s -> UrbanAction(0.))\n",
    "hr = HistoryRecorder(rng=rng, max_steps=400)\n",
    "@time hist = simulate(hr, pomdp, policy, up, b0, s0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"render_helpers.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "function AutomotivePOMDPs.animate_hist(pomdp::UrbanPOMDP, \n",
    "                                         scenes::Vector{Scene}, \n",
    "                                         observations::Vector{Vector{Float64}}, \n",
    "                                         beliefs::Vector{PedCarRNNBelief}, \n",
    "                                         actions::Vector{UrbanAction},\n",
    "                                         safe_actions::Vector{Vector{UrbanAction}},\n",
    "                                         driver_models_hist;\n",
    "                                         sim_dt = 0.1,\n",
    "                                         cam = StaticCamera(VecE2(0., -8.), 14.0))\n",
    "    env = pomdp.env \n",
    "    duration = length(scenes)*sim_dt\n",
    "    fps = Int(1/sim_dt)\n",
    "    function render_rec(t, dt)\n",
    "        frame_index = Int(floor(t/dt)) + 1\n",
    "        overlays = SceneOverlay[IDOverlay()]\n",
    "        obs = [veh for veh in obs_to_scene(pomdp, observations[frame_index]) if veh.id != EGO_ID]\n",
    "        obs_overlay = GaussianSensorOverlay(sensor=pomdp.sensor, o=obs, color=MONOKAI[\"color2\"])\n",
    "        push!(overlays, obs_overlay)\n",
    "        occlusion_overlay = OcclusionOverlay(obstacles=mdp.env.obstacles)\n",
    "        push!(overlays, occlusion_overlay)\n",
    "        cp, pp = 0., 0.\n",
    "        for pred in beliefs[frame_index].predictions\n",
    "            bb, car_pres, ped_pres = process_prediction(pomdp, pred, beliefs[frame_index].obs)\n",
    "            cp += car_pres\n",
    "            pp += ped_pres\n",
    "            bel = [veh for veh in obs_to_scene(pomdp, bb) if veh.id != EGO_ID]\n",
    "            itp_overlay = InterpolationOverlay(mdp, driver_models_hist[frame_index], obs_to_scene(rnn_pomdp, bb))\n",
    "            bel_overlay = GaussianSensorOverlay(sensor=pomdp.sensor, o=bel, color=MONOKAI[\"color4\"]) \n",
    "            push!(overlays, itp_overlay)           \n",
    "            push!(overlays, bel_overlay)\n",
    "        end\n",
    "        cp /= length(beliefs[frame_index].predictions)\n",
    "        pp /= length(beliefs[frame_index].predictions)\n",
    "        push!(overlays, HistogramOverlay(pos=VecE2(-15., -20.), val=cp, label=\"car\"))\n",
    "        push!(overlays, HistogramOverlay(pos=VecE2(-12., -20.), val=pp, label=\"ped\"))\n",
    "        push!(overlays, TextOverlay(text=[\"Probability of presence\"], pos=VecSE2(-17,-14.), font_size=15, incameraframe=true))\n",
    "        push!(overlays, TextOverlay(text = [\"v: $(get_ego(scenes[frame_index]).state.v)\"], font_size=20, \n",
    "                                    pos=VecE2(pomdp.env.params.x_min + 3.,6.), incameraframe=true))\n",
    "        push!(overlays, TextOverlay(text = [\"Acc: $(actions[frame_index].acc)\"], font_size=20,\n",
    "                                    pos=VecE2(pomdp.env.params.x_min + 3.,8.), incameraframe=true))\n",
    "        push!(overlays, TextOverlay(text = [\"Available Actions: $([a.acc for a in safe_actions[frame_index]])\"], font_size=20,\n",
    "                                    pos=VecE2(pomdp.env.params.x_min + 3.,10.), incameraframe=true))\n",
    "        push!(overlays, TextOverlay(text = [\"step: $frame_index\"], font_size=20,\n",
    "                                            pos=VecE2(pomdp.env.params.x_min + 3.,4.), incameraframe=true))\n",
    "                                \n",
    "        return AutoViz.render(scenes[frame_index], env, overlays, cam=cam)\n",
    "    end\n",
    "    return duration, fps, render_rec\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video autoplay controls><source src=\"files/reel-7503070549514879576.webm?9344540700245111012\" type=\"video/webm\"></video>"
      ],
      "text/plain": [
       "Frames{MIME{Symbol(\"image/png\")}}(\"/tmp/tmp5FesMJ\", 0x0000000000000100, 10.0, nothing)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualize the simulation\n",
    "state_history = state_hist(hist)\n",
    "action_history = action_hist(hist)\n",
    "safe_actions_hist = [ai[1] for ai in ainfo_hist(hist)]\n",
    "observation_history = observation_hist(hist)\n",
    "driver_models_hist = info_hist(hist)\n",
    "push!(driver_models_hist, info_hist(hist)[end])\n",
    "insert!(observation_history, 1, o0)\n",
    "belief_history = belief_hist(hist)\n",
    "push!(action_history, UrbanAction(NaN))\n",
    "push!(safe_actions_hist, UrbanAction[])\n",
    "duration, fps, render_hist = animate_hist(pomdp, state_history, observation_history, belief_history, \n",
    "                                          action_history, safe_actions_hist, driver_models_hist)\n",
    "speed_factor = 1\n",
    "film = roll(render_hist, fps = speed_factor*fps, duration = duration/speed_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evaluation_loop (generic function with 3 methods)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function evaluation_loop(pomdp::UrbanPOMDP, policy::Policy, up::PedCarRNNUpdater; n_ep::Int64 = 1000, max_steps::Int64 = 500, rng::AbstractRNG = Base.GLOBAL_RNG)\n",
    "    rewards = zeros(n_ep)\n",
    "    steps = zeros(n_ep)\n",
    "    violations = zeros(n_ep)\n",
    "    @showprogress for ep=1:n_ep\n",
    "        reset_updater!(up)\n",
    "        DeepQLearning.reset_hidden_state!(policy)\n",
    "        s0 = initialstate(pomdp, rng)\n",
    "        a0 = UrbanAction(1.0)\n",
    "        o0 = generate_o(pomdp, s0, a0, s0, rng)\n",
    "        b0 = update(up, PedCarRNNBelief(Vector{Vector{Float64}}(undef, n_models), o0), a0, o0);\n",
    "        singleaction_policy = FunctionPolicy(s -> UrbanAction(0.))\n",
    "        hr = HistoryRecorder(rng=rng, max_steps=max_steps)\n",
    "        hist = simulate(hr, pomdp, policy, up, b0, s0);\n",
    "        rewards[ep] = discounted_reward(hist)\n",
    "        steps[ep] = n_steps(hist)\n",
    "        violations[ep] = is_crash(hist.state_hist[end])#sum(hist.reward_hist .<= -1.) #+ Int(n_steps(hist) >= max_steps)\n",
    "    end\n",
    "    return rewards, steps, violations\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress: 100%|█████████████████████████████████████████|  ETA: 0:00:01\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1261.900106 seconds (4.24 G allocations: 316.537 GiB, 13.80% gc time)\n",
      "Summary for 1000 episodes: \n",
      "Average reward: 0.023 \n",
      "Average # of steps: 93.204 \n",
      "Average # of violations: 1.700 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:21:02\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "up = PedCarRNNUpdater(models, mdp, rnn_pomdp)\n",
    "@time rewards_mask, steps_mask, violations_mask = evaluation_loop(pomdp, policy, up, n_ep=1000, max_steps=400, rng=rng);\n",
    "print_summary(rewards_mask, steps_mask, violations_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Crash detection:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:   2%|█                                        |  ETA: 9:15:22\u001b[39mm"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crash\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32mProgress:   2%|█                                        |  ETA: 9:15:01\u001b[39m\r",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:12:29\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "@showprogress for ep=1:10000\n",
    "    global hist\n",
    "    up = PedCarRNNUpdater(models, mdp, rnn_pomdp)\n",
    "    reset_updater!(up)\n",
    "    DeepQLearning.reset_hidden_state!(policy)\n",
    "    s0 = initialstate(pomdp, rng)\n",
    "    a0 = UrbanAction(1.0)\n",
    "    o0 = generate_o(pomdp, s0, a0, s0, rng)\n",
    "    b0 = update(up, PedCarRNNBelief(Vector{Vector{Float64}}(undef, n_models), o0), a0, o0);\n",
    "    hist = simulate(hr, pomdp, policy, up, b0, s0)\n",
    "    if sum(hist.reward_hist .< 0.) != 0.\n",
    "        println(\"Crash\")\n",
    "        break\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist2.reward_hist[end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simulation with perfect observation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PerfectSensor()"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pomdp.sensor = PerfectSensor();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.191908 seconds (857.43 k allocations: 33.055 MiB, 14.02% gc time)\n"
     ]
    }
   ],
   "source": [
    "up = PerfectSensorUpdater(dqn_pomdp)\n",
    "DeepQLearning.reset_hidden_state!(policy)\n",
    "s0 = initialstate(pomdp, rng)\n",
    "a0 = UrbanAction(1.0)\n",
    "o0 = generate_o(pomdp, s0, a0, s0, rng)\n",
    "b0 = update(up, o0, a0, o0)\n",
    "singleaction_policy = FunctionPolicy(s -> UrbanAction(0.))\n",
    "hr = HistoryRecorder(rng=rng, max_steps=400)\n",
    "@time hist = simulate(hr, pomdp, policy, up, b0, s0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video autoplay controls><source src=\"files/reel-3937198213732385379.webm?4487677827039628990\" type=\"video/webm\"></video>"
      ],
      "text/plain": [
       "Frames{MIME{Symbol(\"image/png\")}}(\"/tmp/tmpd8H2Ek\", 0x0000000000000041, 10.0, nothing)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_history = state_hist(hist)\n",
    "action_history = action_hist(hist)\n",
    "safe_actions_hist = ainfo_hist(hist)\n",
    "observation_history = observation_hist(hist)\n",
    "insert!(observation_history, 1, o0)\n",
    "belief_history = belief_hist(hist)\n",
    "push!(action_history, UrbanAction(NaN))\n",
    "push!(safe_actions_hist, UrbanAction[])\n",
    "duration, fps, render_hist = animate_hist(pomdp, state_history, observation_history, action_history, safe_actions_hist)\n",
    "speed_factor = 1\n",
    "film = roll(render_hist, fps = speed_factor*fps, duration = duration/speed_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simulation with multiple cars and pedestrians**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.868439 seconds (3.82 M allocations: 112.693 MiB, 5.61% gc time)\n"
     ]
    }
   ],
   "source": [
    "pomdp.max_cars = 5\n",
    "pomdp.max_peds = 5\n",
    "still_policy = FunctionPolicy(s -> UrbanAction(0.))\n",
    "up = NothingUpdater()\n",
    "s0 = initialstate(pomdp, rng)\n",
    "hr = HistoryRecorder(rng=rng, max_steps=400)\n",
    "@time hist = simulate(hr, pomdp, still_policy, up, b0, s0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video autoplay controls><source src=\"files/reel-1070636769396691234.webm?15772733053729999738\" type=\"video/webm\"></video>"
      ],
      "text/plain": [
       "Frames{MIME{Symbol(\"image/png\")}}(\"/tmp/tmp1deaqo\", 0x0000000000000191, 10.0, nothing)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duration, fps, render_rec = animate_scenes(hist.state_hist, pomdp.env, sim_dt=pomdp.ΔT, cam = StaticCamera(VecE2(0., -8.), 14.0))\n",
    "speed_factor = 1\n",
    "film = roll(render_rec, fps = speed_factor*fps, duration = duration/speed_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "401"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length(hist.state_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Array{Float64,1}:\n",
       " 0.9999913828130208\n",
       " 0.9999913828130208\n",
       " 0.9999913828130208\n",
       " 0.9922843380022387"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = state_history[1]\n",
    "o = observation_history[1]\n",
    "p_sa = compute_probas(pomdp, policy.mask, o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "function AutomotivePOMDPs.animate_hist(pomdp::UrbanPOMDP, \n",
    "                                         scenes::Vector{Scene}, \n",
    "                                         observations::Vector{Vector{Float64}}, \n",
    "                                         beliefs::Vector{PedCarRNNBelief}, \n",
    "                                         actions::Vector{UrbanAction},\n",
    "                                         safe_actions::Vector{Any};\n",
    "                                         sim_dt = 0.1,\n",
    "                                         cam = StaticCamera(VecE2(0., -8.), 14.0))\n",
    "    env = pomdp.env \n",
    "    duration = length(scenes)*sim_dt\n",
    "    fps = Int(1/sim_dt)\n",
    "    function render_rec(t, dt)\n",
    "        frame_index = Int(floor(t/dt)) + 1\n",
    "        overlays = SceneOverlay[IDOverlay()]\n",
    "        obs = [veh for veh in obs_to_scene(pomdp, observations[frame_index]) if veh.id != EGO_ID]\n",
    "        obs_overlay = GaussianSensorOverlay(sensor=pomdp.sensor, o=obs, color=MONOKAI[\"color2\"])\n",
    "        push!(overlays, obs_overlay)\n",
    "        occlusion_overlay = OcclusionOverlay(obstacles=mdp.env.obstacles)\n",
    "        push!(overlays, occlusion_overlay)\n",
    "        cp, pp = 0., 0.\n",
    "        for pred in beliefs[frame_index].predictions\n",
    "            bb, car_pres, ped_pres = process_prediction(pomdp, pred, beliefs[frame_index].obs)\n",
    "            cp += car_pres\n",
    "            pp += ped_pres\n",
    "            bel = [veh for veh in obs_to_scene(pomdp, bb) if veh.id != EGO_ID]\n",
    "            bel_overlay = GaussianSensorOverlay(sensor=pomdp.sensor, o=bel, color=MONOKAI[\"color4\"])            \n",
    "            push!(overlays, bel_overlay)\n",
    "        end\n",
    "        cp /= length(beliefs[frame_index].predictions)\n",
    "        pp /= length(beliefs[frame_index].predictions)\n",
    "        push!(overlays, HistogramOverlay(pos=VecE2(-15., -20.), val=cp, label=\"car\"))\n",
    "        push!(overlays, HistogramOverlay(pos=VecE2(-12., -20.), val=pp, label=\"ped\"))\n",
    "        push!(overlays, TextOverlay(text=[\"Probability of presence\"], pos=VecSE2(-17,-14.), font_size=15, incameraframe=true))\n",
    "        push!(overlays, TextOverlay(text = [\"v: $(get_ego(scenes[frame_index]).state.v)\"], font_size=20, \n",
    "                                    pos=VecE2(pomdp.env.params.x_min + 3.,6.), incameraframe=true))\n",
    "        push!(overlays, TextOverlay(text = [\"Acc: $(actions[frame_index].acc)\"], font_size=20,\n",
    "                                    pos=VecE2(pomdp.env.params.x_min + 3.,8.), incameraframe=true))\n",
    "        push!(overlays, TextOverlay(text = [\"Available Actions: $([a.acc for a in safe_actions[frame_index]])\"], font_size=20,\n",
    "                                    pos=VecE2(pomdp.env.params.x_min + 3.,10.), incameraframe=true))\n",
    "        push!(overlays, TextOverlay(text = [\"step: $frame_index\"], font_size=20,\n",
    "                                            pos=VecE2(pomdp.env.params.x_min + 3.,4.), incameraframe=true))\n",
    "                                \n",
    "        return AutoViz.render(scenes[frame_index], env, overlays, cam=cam)\n",
    "    end\n",
    "    return duration, fps, render_rec\n",
    "end\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.0",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
